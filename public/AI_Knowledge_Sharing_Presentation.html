<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI & LLM Knowledge Sharing</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            overflow: hidden;
        }

        .presentation-container {
            width: 100vw;
            height: 100vh;
            display: flex;
            flex-direction: column;
        }

        .slide {
            display: none;
            padding: 60px;
            height: 100vh;
            overflow-y: auto;
        }

        .slide.active {
            display: block;
            animation: slideIn 0.5s ease-in-out;
        }

        @keyframes slideIn {
            from { opacity: 0; transform: translateX(30px); }
            to { opacity: 1; transform: translateX(0); }
        }

        h1 {
            font-size: 3.5rem;
            text-align: center;
            margin-bottom: 30px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        h2 {
            font-size: 2.5rem;
            margin-bottom: 40px;
            color: #FFE066;
            text-shadow: 1px 1px 2px rgba(0,0,0,0.3);
        }

        h3 {
            font-size: 1.8rem;
            margin-bottom: 20px;
            color: #B8E6B8;
        }

        .content {
            font-size: 1.3rem;
            line-height: 1.8;
            max-width: 1000px;
            margin: 0 auto;
        }

        .two-column {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 40px;
            margin-top: 30px;
        }

        .highlight {
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 5px solid #FFE066;
        }

        .code-block {
            background: rgba(0, 0, 0, 0.3);
            padding: 20px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 1.1rem;
            margin: 20px 0;
            overflow-x: auto;
        }

        ul, ol {
            margin-left: 30px;
            margin-bottom: 20px;
        }

        li {
            margin-bottom: 10px;
            font-size: 1.2rem;
        }

        .emoji {
            font-size: 1.5em;
            margin-right: 10px;
        }

        .navigation {
            position: fixed;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 20px;
            align-items: center;
            background: rgba(0, 0, 0, 0.3);
            padding: 15px 25px;
            border-radius: 25px;
        }

        .nav-btn {
            background: rgba(255, 255, 255, 0.2);
            border: none;
            color: white;
            padding: 10px 20px;
            border-radius: 20px;
            cursor: pointer;
            font-size: 1rem;
            transition: all 0.3s ease;
        }

        .nav-btn:hover {
            background: rgba(255, 255, 255, 0.3);
            transform: scale(1.05);
        }

        .nav-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .slide-counter {
            color: white;
            font-weight: bold;
        }

        .slide-select {
            background: rgba(255, 255, 255, 0.2);
            border: none;
            color: white;
            padding: 8px 15px;
            border-radius: 15px;
            font-size: 0.9rem;
        }

        .comparison-table {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
        }

        .comparison-table table {
            width: 100%;
            border-collapse: collapse;
        }

        .comparison-table th,
        .comparison-table td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid rgba(255, 255, 255, 0.2);
        }

        .comparison-table th {
            background: rgba(255, 255, 255, 0.1);
            font-weight: bold;
            color: #FFE066;
        }

        .demo-section {
            background: rgba(255, 255, 255, 0.05);
            padding: 30px;
            border-radius: 15px;
            margin: 30px 0;
            border: 2px solid rgba(255, 255, 255, 0.1);
        }

        .warning {
            background: rgba(255, 152, 0, 0.2);
            border-left: 5px solid #FF9800;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
        }

        .success {
            background: rgba(76, 175, 80, 0.2);
            border-left: 5px solid #4CAF50;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <div class="presentation-container">
    <!-- Slide 1: Title -->
        <div class="slide active">
            <div class="content">
                <h1>ü§ñ AI & LLM Knowledge Sharing</h1>
                <div style="text-align: center; margin-top: 50px;">
                    <h2>Understanding Large Language Models</h2>
                    <p style="font-size: 1.5rem; margin-top: 30px;">Local vs Cloud ‚Ä¢ RAG ‚Ä¢ Practical Tools</p>
                    <p style="font-size: 1.2rem; margin-top: 40px; opacity: 0.8;">
                        Backend Track - AI Integration Deep Dive
                    </p>
                </div>
            </div>
        </div>

        <!-- Slide 2: What are LLMs? -->
        <div class="slide">
            <div class="content">
                <h2>üß† What are Large Language Models (LLMs)?</h2>
                
                <div class="highlight">
                    <h3>Definition</h3>
                    <p>Neural networks trained on massive text datasets to understand and generate human-like text</p>
    </div>

                <div class="two-column">
                <div>
                        <h3>üîß How They Work</h3>
                        <ul>
                            <li><strong>Transformer Architecture:</strong> Neural network design that processes entire sequences at once</li>
                            <li><strong>Attention Mechanisms:</strong> Focus on relevant parts of input text when generating responses</li>
                            <li><strong>Token-based Processing:</strong> Break text into smaller units (words, parts of words, punctuation)</li>
                            <li><strong>Statistical Pattern Recognition:</strong> Learn relationships between tokens from massive training data</li>
                            <li><strong>Contextual Understanding:</strong> Consider surrounding words to determine meaning</li>
                        </ul>
                </div>
                <div>
                        <h3>üí° Capabilities</h3>
                        <ul>
                            <li>Text generation & completion</li>
                            <li>Language translation</li>
                            <li>Code generation & debugging</li>
                            <li>Question answering</li>
                            <li>Content summarization</li>
                        </ul>
                        </div>
                        </div>

                <div class="warning">
                    <strong>Key Point:</strong> LLMs predict the next most likely token based on patterns learned from training data
                        </div>
                        </div>
                    </div>

        <!-- Slide 3: Understanding LLM Parameters -->
        <div class="slide">
            <div class="content">
                <h2>üî¢ Understanding LLM Parameters: The Numbers Game</h2>
                
                <div class="highlight">
                    <h3>üí° What is a "Model" in LLMs?</h3>
                    <p><strong>Model</strong> refers to the complete trained neural network - it's the "brain" that has learned to understand and generate language. Think of it as the entire software program that can process text.</p>
                </div>

                <div class="highlight" style="margin-top: 20px;">
                    <h3>üî¢ What "Parameters" Mean in LLMs</h3>
                    <p><strong>Parameters</strong> are the adjustable values (weights and biases) in the neural network that store the model's learned knowledge. Think of them as the model's "memory cells" that hold patterns from training data.</p>
                </div>

                <div class="two-column">
                    <div>
                        <h3>üìä Parameter Size Examples</h3>
                        <ul>
                            <li><strong>7B (7 Billion):</strong> GPT-3, LLaMA-2-7B</li>
                            <li><strong>13B (13 Billion):</strong> LLaMA-2-13B, Vicuna-13B</li>
                            <li><strong>70B (70 Billion):</strong> LLaMA-2-70B, GPT-3.5</li>
                            <li><strong>175B (175 Billion):</strong> GPT-3 (original)</li>
                            <li><strong>1.7T (1.7 Trillion):</strong> GPT-4 (estimated)</li>
                        </ul>

                        <div class="code-block">
# Parameter Count Impact
Smaller Models (7B-13B):
- Faster inference
- Lower memory usage
- Good for basic tasks

Larger Models (70B+):
- Better reasoning
- More nuanced responses
- Higher resource requirements
                        </div>
                    </div>
                    
                    <div>
                        <h3>‚ùì Why the Numbers Differ</h3>
                        <ul>
                            <li><strong>Architecture:</strong> Different model designs</li>
                            <li><strong>Training Data:</strong> Amount and quality of training</li>
                            <li><strong>Optimization:</strong> Model compression techniques</li>
                            <li><strong>Task Focus:</strong> General vs specialized models</li>
                            <li><strong>Efficiency:</strong> Newer models do more with fewer params</li>
                        </ul>

                        <h3>üéØ Rule of Thumb</h3>
                        <div class="success">
                            <ul>
                                <li><strong>7B-13B:</strong> Good for basic tasks, local deployment</li>
                                <li><strong>30B-70B:</strong> Sweet spot for most applications</li>
                                <li><strong>100B+:</strong> Best performance, requires cloud/GPU</li>
                                <li><strong>Parameter count ‚â† Quality:</strong> Newer models often outperform older, larger ones</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="warning">
                    <strong>Important:</strong> More parameters don't always mean better performance. Modern models use techniques like better training data, improved architectures, and optimization to achieve superior results with fewer parameters.
                </div>
            </div>
        </div>

        <!-- Slide 4: Popular LLM Models -->
        <div class="slide">
            <div class="content">
                <h2>üåü Popular LLM Models & Providers</h2>
                
                <div class="comparison-table">
                    <table>
                        <thead>
                            <tr>
                                <th>Model</th>
                                <th>Provider</th>
                                <th>Type</th>
                                <th>Best For</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>GPT-4</strong></td>
                                <td>OpenAI</td>
                                <td>Cloud API</td>
                                <td>Complex reasoning, coding</td>
                            </tr>
                            <tr>
                                <td><strong>Gemini Pro</strong></td>
                                <td>Google</td>
                                <td>Cloud API</td>
                                <td>Multimodal, fast responses</td>
                            </tr>
                            <tr>
                                <td><strong>Claude</strong></td>
                                <td>Anthropic</td>
                                <td>Cloud API</td>
                                <td>Long context, safety</td>
                            </tr>
                            <tr>
                                <td><strong>LLaMA 2/3</strong></td>
                                <td>Meta</td>
                                <td>Open Source</td>
                                <td>Local deployment, customization</td>
                            </tr>
                            <tr>
                                <td><strong>Mistral</strong></td>
                                <td>Mistral AI</td>
                                <td>Open Source</td>
                                <td>Efficiency, multilingual</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="highlight">
                    <h3>üéØ Our Project Choice: Google Gemini</h3>
                    <p>We chose Gemini for our Laravel AI Log Analyzer due to its fast API responses, good reasoning capabilities, and generous free tier for development.</p>
            </div>
        </div>
    </div>

        <!-- Slide 4: LLaMA - Local LLM Powerhouse -->
        <div class="slide">
            <div class="content">
                <h2>ü¶ô LLaMA: The Local LLM Revolution</h2>
                
                <div class="two-column">
                <div>
                        <h3>üî• What is LLaMA?</h3>
                        <ul>
                            <li><strong>Meta's open-source LLM</strong></li>
                            <li>Available in 7B, 13B, 70B parameters</li>
                            <li>LLaMA 2 & 3 with commercial license</li>
                            <li>Can run entirely offline</li>
                            <li>No API costs after setup</li>
                        </ul>

                        <h3>‚ö° Performance</h3>
                        <ul>
                            <li>Competitive with GPT-3.5</li>
                            <li>LLaMA 3 rivals GPT-4 on many tasks</li>
                            <li>Efficient inference</li>
                            <li>Quantized versions for mobile</li>
                        </ul>
                </div>
                <div>
                        <h3>üõ†Ô∏è Running LLaMA Locally</h3>
                        <div class="code-block">
# Using Ollama (easiest way)
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull llama3:8b
ollama run llama3:8b

# Using Python (Transformers)
pip install transformers torch
from transformers import LlamaForCausalLM
model = LlamaForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
                    </div>
                    
                        <h3>üìä System Requirements</h3>
                        <ul>
                            <li><strong>7B model:</strong> 8GB RAM</li>
                            <li><strong>13B model:</strong> 16GB RAM</li>
                            <li><strong>70B model:</strong> 48GB+ RAM</li>
                            <li>GPU acceleration recommended</li>
                        </ul>
                        </div>
                        </div>
                        </div>
                        </div>

        <!-- Slide 5: Local vs Cloud LLMs -->
        <div class="slide">
            <div class="content">
                <h2>üîÑ Local vs Cloud LLMs: The Trade-offs</h2>
                
                <div class="two-column">
                    <div>
                        <div class="success">
                            <h3>üè† Local LLMs (LLaMA, Ollama)</h3>
                            <h4>‚úÖ Advantages:</h4>
                            <ul>
                                <li><strong>Privacy:</strong> Data never leaves your server</li>
                                <li><strong>Cost:</strong> No API fees after setup</li>
                                <li><strong>Control:</strong> Full model customization</li>
                                <li><strong>Offline:</strong> Works without internet</li>
                                <li><strong>No Rate Limits:</strong> Unlimited requests</li>
                                <li><strong>Compliance:</strong> Meet strict data requirements</li>
                            </ul>
                            
                            <h4>‚ùå Challenges:</h4>
                            <ul>
                                <li>Hardware requirements</li>
                                <li>Setup complexity</li>
                                <li>Slower inference</li>
                                <li>Model management</li>
                            </ul>
        </div>
    </div>

                <div>
                        <div class="highlight">
                            <h3>‚òÅÔ∏è Cloud LLMs (GPT, Gemini)</h3>
                            <h4>‚úÖ Advantages:</h4>
                            <ul>
                                <li><strong>Easy Setup:</strong> Just API keys</li>
                                <li><strong>Performance:</strong> Latest, most powerful models</li>
                                <li><strong>Speed:</strong> Optimized infrastructure</li>
                                <li><strong>Updates:</strong> Automatic improvements</li>
                                <li><strong>Scale:</strong> Handle any load</li>
                            </ul>
                            
                            <h4>‚ùå Challenges:</h4>
                            <ul>
                                <li>Ongoing API costs</li>
                                <li>Privacy concerns</li>
                                <li>Rate limits</li>
                                <li>Internet dependency</li>
                                <li>Vendor lock-in</li>
                    </ul>
                </div>
                        </div>
                        </div>

                <div class="warning">
                    <strong>Recommendation:</strong> Start with cloud APIs for prototyping, consider local deployment for production with sensitive data
            </div>
        </div>
    </div>

        <!-- Slide 6: RAG - Retrieval Augmented Generation -->
        <div class="slide">
            <div class="content">
                <h2>üîç RAG: Retrieval Augmented Generation</h2>
                
                <div class="highlight">
                    <h3>üí° What is RAG?</h3>
                    <p>A technique that combines LLMs with external knowledge retrieval to provide accurate, up-to-date, and context-specific responses.</p>
    </div>

                <div class="two-column">
                <div>
                        <h3>üîß How RAG Works</h3>
                        <ol>
                            <li><strong>Document Ingestion:</strong> Split documents into chunks</li>
                            <li><strong>Embedding Creation:</strong> Convert chunks to vectors</li>
                            <li><strong>Vector Storage:</strong> Store in vector database</li>
                            <li><strong>Query Processing:</strong> Convert user query to vector</li>
                            <li><strong>Similarity Search:</strong> Find relevant chunks</li>
                            <li><strong>Context Injection:</strong> Add context to LLM prompt</li>
                            <li><strong>Generation:</strong> LLM generates informed response</li>
                        </ol>
                    </div>
                    
                <div>
                        <h3>üéØ RAG Benefits</h3>
                        <ul>
                            <li><strong>Accuracy:</strong> Factual, source-backed answers</li>
                            <li><strong>Freshness:</strong> Use latest information</li>
                            <li><strong>Specificity:</strong> Domain-specific knowledge</li>
                            <li><strong>Transparency:</strong> Cite sources</li>
                            <li><strong>Cost-Effective:</strong> No model retraining needed</li>
                        </ul>

                        <div class="code-block">
# RAG Architecture
User Query ‚Üí Embedding Model ‚Üí 
Vector DB Search ‚Üí Relevant Docs ‚Üí 
LLM + Context ‚Üí Enhanced Answer
                        </div>
                        </div>
                    </div>
                    
                <div class="success">
                    <strong>Real-World Example:</strong> Our log analyzer could use RAG to search through historical logs and provide context-aware explanations based on past patterns and solutions.
            </div>
        </div>
    </div>



        <!-- Slide 7: Practical AI Tools for Development -->
        <div class="slide">
            <div class="content">
                <h2>üöÄ Practical AI Tools for Developers</h2>
                
                <div class="two-column">
                <div>
                        <h3>üíª Code Generation & Assistance</h3>
                        <ul>
                            <li><strong>GitHub Copilot:</strong> AI pair programmer</li>
                            <li><strong>Cursor:</strong> AI-first code editor</li>
                            <li><strong>CodeWhisperer:</strong> Amazon's code assistant</li>
                            <li><strong>Tabnine:</strong> AI autocomplete</li>
                            <li><strong>Replit Ghostwriter:</strong> Cloud-based coding AI</li>
                        </ul>

                        <h3>üêõ Debugging & Testing</h3>
                        <ul>
                            <li><strong>DeepCode:</strong> AI code review</li>
                            <li><strong>Snyk:</strong> Security vulnerability detection</li>
                            <li><strong>Testim:</strong> AI-powered test automation</li>
                            <li><strong>Applitools:</strong> Visual testing with AI</li>
                        </ul>
                        </div>
                    
                <div>
                        <h3>üìä API & Integration Tools</h3>
                        <ul>
                            <li><strong>OpenAI API:</strong> GPT-4, DALL-E, Whisper</li>
                            <li><strong>Anthropic Claude:</strong> Constitutional AI</li>
                            <li><strong>Google Gemini:</strong> Multimodal capabilities</li>
                            <li><strong>Hugging Face:</strong> Open source models</li>
                            <li><strong>Replicate:</strong> Run models via API</li>
                        </ul>

                        <h3>üîß Local Development</h3>
                        <ul>
                            <li><strong>Ollama:</strong> Run LLMs locally</li>
                            <li><strong>LM Studio:</strong> GUI for local models</li>
                            <li><strong>Text Generation WebUI:</strong> Gradio interface</li>
                            <li><strong>LocalAI:</strong> OpenAI-compatible local API</li>
                        </ul>
                            </div>
                        </div>

                <div class="warning">
                    <strong>Pro Tip:</strong> Start with cloud APIs for rapid prototyping, then evaluate local deployment for cost optimization and privacy requirements.
                            </div>
                        </div>
                            </div>

        <!-- Slide 8: Our Laravel AI Implementation -->
        <div class="slide">
            <div class="content">
                <h2>üîß Our Laravel AI Log Analyzer</h2>
                
                <div class="demo-section">
                    <h3>üéØ Project Overview</h3>
                    <p>Built a Laravel application that integrates Google Gemini API for intelligent log analysis and SQL optimization.</p>
                        </div>

                <div class="two-column">
                            <div>
                        <h3>üèóÔ∏è Architecture</h3>
                        <ul>
                            <li><strong>Laravel Framework:</strong> Web application foundation</li>
                            <li><strong>GeminiService:</strong> AI API integration layer</li>
                            <li><strong>Controllers:</strong> Request handling & routing</li>
                            <li><strong>Frontend:</strong> Modern UI with Tailwind & Alpine.js</li>
                            <li><strong>Config Management:</strong> Environment-based API keys</li>
                        </ul>

                        <h3>üîÑ API Endpoints</h3>
                        <ul>
                            <li><code>/api/ai/explain-log</code></li>
                            <li><code>/api/ai/classify-log</code></li>
                            <li><code>/api/ai/explain-sql</code></li>
                            <li><code>/api/ai/optimize-sql</code></li>
                        </ul>
                        </div>
                    
                    <div>
                <div class="code-block">
// GeminiService Implementation
class GeminiService
{
    protected string $apiKey;
    protected string $model = 'gemini-1.5-flash';
    
public function generateResponse(string $prompt): string
{
        $response = Http::post($this->baseUrl . '/models/' . 
            $this->model . ':generateContent?key=' . 
            $this->apiKey, [
        'contents' => [
                [
                    'parts' => [
                        ['text' => $prompt]
                    ]
                ]
        ],
        'generationConfig' => [
            'temperature' => 0.3,
            'maxOutputTokens' => 1000,
        ]
    ]);
    
        return $response->json()['candidates'][0]
            ['content']['parts'][0]['text'];
    }
}
                        </div>
                </div>
            </div>
        </div>
    </div>



        <!-- Slide 9: Critical Question - Productivity -->
        <div class="slide">
            <div class="content">
                <h1 style="text-align: center; margin-bottom: 80px;">ü§î Critical Question</h1>
                
                <div style="text-align: center;">
                    <div class="highlight" style="font-size: 2.5rem; padding: 60px; margin: 40px 0;">
                        <strong>Make Sure the AI Tool Makes You More Productive</strong>
                    </div>
          
                </div>
            </div>
        </div>

        <!-- Slide 10: METR Study Results -->
        <div class="slide">
            <div class="content">
                <h2>üìä Reality Check: Recent Scientific Study on AI Tool Productivity</h2>
                
                <div class="warning" style="margin: 30px 0; font-size: 1.2rem;">
                    <strong>METR Study (2025):</strong> AI tools made experienced developers 19% slower in real-world tasks
                </div>

                <div class="two-column">
                    <div>
                        <h3>üî¨ Study Details</h3>
                        <ul>
                            <li><strong>Participants:</strong> 16 professional developers</li>
                            <li><strong>Duration:</strong> February - June 2025</li>
                            <li><strong>Context:</strong> Real open-source projects they knew well</li>
                            <li><strong>Tasks:</strong> Bug fixes, feature additions, modifications</li>
                            <li><strong>Tools Tested:</strong> Cursor Pro, Claude 3.5/3.7 Sonnet</li>
                        </ul>

                        <h3>üìà Surprising Results</h3>
                        <ul>
                            <li><strong>Actual Performance:</strong> 19% slower with AI</li>
                            <li><strong>Perceived Performance:</strong> 20-24% faster</li>
                            <li><strong>Gap:</strong> Developers felt more productive but weren't</li>
                        </ul>
                    </div>
                    
                    <div>
                        <h3>‚è±Ô∏è Where Time Was Lost</h3>
                        <ul>
                            <li><strong>Prompt Engineering:</strong> Writing and experimenting with prompts</li>
                            <li><strong>Waiting:</strong> Time spent waiting for AI responses</li>
                            <li><strong>Code Review:</strong> Reviewing and fixing AI-generated code</li>
                            <li><strong>Context Switching:</strong> Moving between AI tool and editor</li>
                            <li><strong>Debugging AI Output:</strong> Understanding and correcting suggestions</li>
                        </ul>

                        <div class="highlight">
                            <h3>üéØ Key Insight</h3>
                            <p>AI effectiveness depends on:</p>
                            <ul>
                                <li>Developer experience level</li>
                                <li>Task complexity</li>
                                <li>Codebase familiarity</li>
                                <li>How the tool is used</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="success">
                    <strong>Takeaway:</strong> AI tools aren't automatically better for everyone. Measure your actual productivity, not just your feeling of productivity. Consider when and how to use AI tools strategically.
                </div>
            </div>
        </div>



        <!-- Slide 11: Thank You -->
        <div class="slide">
            <div class="content">
                <h1>üéâ Let's Build the AI Future Together!</h1>
                <div style="text-align: center; margin-top: 80px;">
                    <h2>Key Takeaways</h2>
                    <div class="highlight" style="margin: 40px 0; font-size: 1.3rem;">
                        <ul style="text-align: left; max-width: 800px; margin: 0 auto;">
                            <li>ü¶ô <strong>Local LLMs (LLaMA)</strong> offer privacy & cost benefits</li>
                            <li>‚òÅÔ∏è <strong>Cloud APIs</strong> provide easy setup & latest models</li>
                            <li>üîç <strong>RAG</strong> enhances AI with your own knowledge base</li>
                            <li>üõ†Ô∏è <strong>Practical tools</strong> can transform daily development</li>
                            <li>ü§ù <strong>Cross-team collaboration</strong> multiplies AI impact</li>
                        </ul>
    </div>

                    <p style="font-size: 1.4rem; margin-top: 60px;">
                        <strong>Ready to explore AI integration in your projects?</strong>
                    </p>
                    
                    <p style="font-size: 1.1rem; margin-top: 30px; opacity: 0.8;">
                        Questions? Let's discuss how AI can enhance your specific use cases!
                    </p>
            </div>
        </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" id="prevBtn" onclick="changeSlide(-1)">‚Äπ Previous</button>
        
        <select class="slide-select" id="slideSelect" onchange="goToSlide(this.value)">
            <option value="0">Title - AI & LLM Knowledge Sharing</option>
            <option value="1">What are LLMs?</option>
            <option value="2">Understanding LLM Parameters</option>
            <option value="3">Popular LLM Models</option>
            <option value="4">LLaMA - Local LLM Powerhouse</option>
            <option value="5">Local vs Cloud LLMs</option>
            <option value="6">RAG - Retrieval Augmented Generation</option>
            <option value="7">Practical AI Tools for Development</option>
            <option value="8">Our Laravel AI Implementation</option>
            <option value="9">Critical Question - Productivity</option>
            <option value="10">METR Study Results</option>
            <option value="11">Thank You</option>
        </select>

        <span class="slide-counter">
            <span id="currentSlide">1</span> / <span id="totalSlides">12</span>
        </span>
        
        <button class="nav-btn" id="nextBtn" onclick="changeSlide(1)">Next ‚Ä∫</button>
    </div>

    <script>
        let currentSlideIndex = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;

        document.getElementById('totalSlides').textContent = totalSlides;

        function showSlide(index) {
            slides.forEach((slide, i) => {
                slide.classList.toggle('active', i === index);
            });
            
            currentSlideIndex = index;
            document.getElementById('currentSlide').textContent = index + 1;
            document.getElementById('slideSelect').value = index;
            
            // Update navigation buttons
            document.getElementById('prevBtn').disabled = index === 0;
            document.getElementById('nextBtn').disabled = index === totalSlides - 1;
        }

        function changeSlide(direction) {
            const newIndex = currentSlideIndex + direction;
            if (newIndex >= 0 && newIndex < totalSlides) {
                showSlide(newIndex);
            }
        }

        function goToSlide(index) {
            showSlide(parseInt(index));
        }

        // Initialize
        showSlide(0);

        // Keyboard navigation
        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowLeft') changeSlide(-1);
            if (e.key === 'ArrowRight') changeSlide(1);
        });
    </script>
</body>
</html>